{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "___\n",
    "#### Description:\n",
    "\n",
    "Word embeddings are vector representations of words similar to one-hot encodings, but they are better in that they hold semantic meaning. The cosine distance between two similar words will be small whereas the cosine distance between two very different words will be large. The two most popular methods used to learn word embeddings are Word2Vec and GloVe. The papers to these methods can be found here:\n",
    "\n",
    "Word2Vec: https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "GloVe: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "\n",
    "Training good word embeddings using Word2Vec or GloVe requires a lot of data. Fortunately there are many pre-trained word embeddings publicly available online. Using transfer learning, we can take these pre-trained word embeddings and use them in our model. In this case I use 50-dimensional GloVe word embeddings. Note that if we don't use pre-trained word embeddings and simply train a Keras Embedding layer from scratch, the embeddings will likely be inferior for two reasons: it is trained on less data, and the embedding is trained with the goal of trying to minimize the loss of the entire network instead of trying to capture semantics of words. \n",
    "\n",
    "___\n",
    "#### Dataset:\n",
    "\n",
    "The dataset contains 1000 restaurant reviews along with a rating of '0' or '1'. A '1' if the reviewer liked the food and a '0' if they did not. The dataset can be obtained from: https://www.kaggle.com/hj5992/restaurantreviews/data\n",
    "\n",
    "___\n",
    "#### Reference:\n",
    "\n",
    "I use the 50-dimensional word embeddings trained on 6 billion words, which can be downloaded here:\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Also, this resource was used as a helpful guide: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_table('Restaurant_Reviews.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      "Review    1000 non-null object\n",
      "Liked     1000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into reviews and sentiments\n",
    "reviews = df['Review']\n",
    "sentiments = df['Liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although I very much liked the look and sound of this place, the actual experience was a bit disappointing.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a review to get an idea of how to preprocess\n",
    "reviews[np.random.randint(len(reviews))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the GloVe word embeddings as a dictionary\n",
    "embeddings = {}\n",
    "\n",
    "with open('glove.6B/glove.6B.50d.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of original file: 400000\n"
     ]
    }
   ],
   "source": [
    "# We will not be using every word embedding\n",
    "print('Vocab size of original file:', len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean a review\n",
    "def clean_review(review):\n",
    "    review = review.lower() # Make letters lowercase\n",
    "    review.replace('-', ' ') # Separate hyphenated words\n",
    "    review.replace('.', ' ') # Fix phrases like \"salt and pepper..and of course\"\n",
    "    words = review.split() # Split review into words for further cleaning\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = re.sub('[^a-z]', '', word) # Remove non-alphabetical characters\n",
    "        if embeddings.get(word) is not None: # Remove words not in GloVe vocab\n",
    "            new_words.append(word)\n",
    "        \n",
    "    review = ' '.join(new_words) # Put words back together to form clean review\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean every review\n",
    "clean_reviews = [clean_review(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " If there were zero stars I would give it zero stars.\n",
      "\n",
      "Cleaned:\n",
      " if there were zero stars i would give it zero stars\n"
     ]
    }
   ],
   "source": [
    "# Compare an original to a cleaned review\n",
    "index = np.random.randint(len(reviews))\n",
    "\n",
    "print('Original:\\n', reviews[index])\n",
    "print('\\nCleaned:\\n', clean_review(reviews[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and integer encode the reviews\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_reviews)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 46, 27, 327, 114, 3, 51, 185, 9, 327, 114]\n"
     ]
    }
   ],
   "source": [
    "# Display a review in its new representation\n",
    "print(sequences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 32\n"
     ]
    }
   ],
   "source": [
    "# Choose length of sequence (input)\n",
    "maxlen = len(max(sequences, key=len))\n",
    "print('Max length:', maxlen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1958\n"
     ]
    }
   ],
   "source": [
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1 # including 0th index\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to max length\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=maxlen, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix for our vocab using GloVe embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_matrix[i] = embeddings.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vocab size, embedding length) -> (1958, 50)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The embedding matrix contains the embedding for every word in our \n",
    "vocabulary and the zero vector. Each embedding is a vector with 50 \n",
    "values. We can think of it as a matrix where each row represents a \n",
    "word with the exception of the first row which is reserved for the \n",
    "zero vector. It is common to see people use vocabulary sizes much\n",
    "larger but the small dataset used here didn't have that many unique \n",
    "words.\n",
    "\"\"\"\n",
    "\n",
    "print('(vocab size, embedding length) ->', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 32)\n",
      "y shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Get the inputs and outputs ready for training\n",
    "X = sequences\n",
    "y = np.array(sentiments)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 50)            97900     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 118,151\n",
      "Trainable params: 20,251\n",
      "Non-trainable params: 97,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build an RNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "    LSTM(50, dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.6764 - acc: 0.5675 - val_loss: 0.7119 - val_acc: 0.4750\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.6227 - acc: 0.6750 - val_loss: 0.7811 - val_acc: 0.4500\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.5891 - acc: 0.6938 - val_loss: 0.7259 - val_acc: 0.5450\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.5622 - acc: 0.6975 - val_loss: 0.7265 - val_acc: 0.5550\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.5306 - acc: 0.7250 - val_loss: 0.7181 - val_acc: 0.5800\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.5158 - acc: 0.7437 - val_loss: 0.5526 - val_acc: 0.7650\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.5093 - acc: 0.7375 - val_loss: 0.7014 - val_acc: 0.6100\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4913 - acc: 0.7750 - val_loss: 0.6593 - val_acc: 0.6500\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4888 - acc: 0.7700 - val_loss: 0.5689 - val_acc: 0.7500\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4930 - acc: 0.7550 - val_loss: 0.5138 - val_acc: 0.7850\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4683 - acc: 0.7662 - val_loss: 0.5139 - val_acc: 0.7950\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4676 - acc: 0.7688 - val_loss: 0.6590 - val_acc: 0.6800\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4458 - acc: 0.7913 - val_loss: 0.5865 - val_acc: 0.7100\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4340 - acc: 0.7888 - val_loss: 0.6765 - val_acc: 0.6600\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4213 - acc: 0.8100 - val_loss: 0.5392 - val_acc: 0.7600\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4119 - acc: 0.8212 - val_loss: 0.5806 - val_acc: 0.7250\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4001 - acc: 0.8237 - val_loss: 0.5438 - val_acc: 0.7250\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4315 - acc: 0.7987 - val_loss: 0.4628 - val_acc: 0.7800\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.3960 - acc: 0.8325 - val_loss: 0.5035 - val_acc: 0.7700\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.3862 - acc: 0.8325 - val_loss: 0.5643 - val_acc: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2121193feb8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model to X and y\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=20, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
